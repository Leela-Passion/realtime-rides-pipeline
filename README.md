<!-- PROJECT BANNER -->
<p align="center">
  <img src="https://raw.githubusercontent.com/Leela-Passion/realtime-rides-pipeline/main/architecture/architecture_design.png" width="100%" alt="Project Banner" />
</p>

<h1 align="center">ğŸš• Real-Time Ride Sharing Analytics Platform</h1>
<p align="center">
  <b>Databricks â€¢ Delta Lake â€¢ dbt â€¢ Medallion Architecture â€¢ SCD Type-2 â€¢ SQL</b>
</p>

---

## ğŸ“Œ Project Overview
This project builds a **real-time analytics platform for a ride-sharing company**, showcasing the complete **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** on Databricks with **dbt transformations** and **SCD Type-2 snapshots**.

The project demonstrates **end-to-end data engineering skills**:

- Ingestion â†’ Cleaning â†’ Modelling â†’ SCD â†’ Documentation â†’ Quality Tests â†’ Lineage  
- Ready for **Data Engineer**, **Analytics Engineer**, and **Data Analyst** job roles.

---

## ğŸ—ï¸ Architecture Diagram
Below is the architecture used in this project.

> **Architecture Diagram**
<p align="center">
  <img src="https://raw.githubusercontent.com/yourusername/yourrepo/main/architecture/architecture_design.png" width="90%" alt="Architecture Diagram" />
</p>

---

## ğŸ¯ Problem Solved
Ride-sharing platforms generate large volumes of transactional dataâ€”trips, payments, drivers, vehicles, locations, and customer interactions.

This project solves:
- ğŸš€ **Low-latency reporting** using structured streaming ingestion  
- ğŸ“Š **Reliable analytics** using dbt transformations  
- â™»ï¸ **Historical tracking** using SCD Type-2  
- ğŸ§ª **Data quality assurance** using dbt tests  
- ğŸ” **Lineage & documentation** for transparency  

---

## ğŸ¯ **Project Objectives**

This project showcases a scalable data pipeline designed for:

- **Ingestion â†’ Transformation â†’ Modeling â†’ Documentation**
- **SCD Type-2 Snapshots** using dbt
- **Data quality testing** (unique, not_null, relationships)
- **Dimensional modeling** (Gold layer)
- **High-performance transformations** using Spark SQL
- **Interactive documentation** auto-generated by dbt
- **Dashboard-ready data marts** for analytics

It is intentionally designed for **recruiter readability** + **technical depth**.

---

## âš¡ Features

### ğŸŸ¤ **Bronze Layer â€“ Raw Ingestion**
- Raw CSV/JSON-like data stored in Delta tables
- Notebook: `bronze_ingestion.html`

### ğŸŸ¡ **Silver Layer â€“ Cleaning & Conformance**
- Standardized schemas
- Deduplication using window functions  
- Dynamic transformations notebook  
  (`dynamic_transformations-bronze_to_silver.html`)
- Example entity notebook  
  (`silver_transformation-customers_entity.html`)

### ğŸŸ¢ **Gold Layer â€“ Dimensions & Facts**
- dbt models for Customers, Drivers, Vehicles, Trips, Payments  
- Relationship & quality tests applied in `schema.yml`

### ğŸ“˜ **dbt SCD Type-2 Snapshots**
- Snapshot configs stored under `snapshots/SCDs.yml`  
- CDC using `last_updated_timestamp`  
- Temporal validity using `dbt_valid_to_current`

### ğŸ“š **Interactive Documentation**
- Auto-generated via dbt docs  
- Includes lineage graph, model descriptions, tests, and sources

---

## ğŸ§± Technology Stack

| Layer | Tools |
|------|-------|
| Compute | Databricks SQL / Spark |
| Transformation | **dbt** (core / cloud) |
| Storage | Delta Lake |
| Modeling | SCD Type-2, Star Schema |
| Orchestration | dbt Cloud (online, no local setup) |
| Documentation | dbt docs, GitHub Pages |
| Version Control | GitHub |

---

## ğŸš€ How to Run the Project (NO INSTALLATION REQUIRED)

### **Option 1 â€” Run Online Using dbt Cloud (Recommended)**  
âœ¨ Zero installation  
âœ¨ Browser-based execution  
âœ¨ Saves job logs automatically

**Steps:**
1. Create free account â†’ https://cloud.getdbt.com  
2. Connect to Databricks workspace  
3. Import this GitHub repo  
4. Click **Run** â†’ â€œStart Runâ€  
5. Open **Documentation** & **Lineage Graph** inside dbt Cloud

---

### **Option 2 â€” Open Docs on GitHub Pages**  
If enabled, visit:

ğŸ‘‰ `https://Leela-Passion.github.io/realtime-rides-pipeline/`  
Contains:
- Model documentation  
- Column-level metadata  
- Snapshots  
- DAG lineage graph  

---

## ğŸ“‚ Folder Structure (Clean & Recruiter-Friendly)

`
realtime-rides-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ architecture_design.png
|
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze_ingestion.html
â”‚   â”œâ”€â”€ silver_transformation-customers_entity.html
â”‚   â”œâ”€â”€ dynamic_transformations-bronze_to_silver.html
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ source/
|   |       â”œâ”€â”€ sources.yml
|   |       â”œâ”€â”€ schema.yml
â”‚   â”‚   â”œâ”€â”€ silver/
|           â””â”€â”€ trips.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚    â”œâ”€â”€ fact.yml
|   |    â”œâ”€â”€ SCDs.yml    
â”‚   â”œâ”€â”€ macros/
|   |   â””â”€â”€ generate_schema_name.sql
â”‚   
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ overview.md
    â”œâ”€â”€ data_dictionary.md
    â”œâ”€â”€ dashboard_screenshots/
`

---

## ğŸ§ª dbt Commands Used in CI (no local install needed)

```bash
dbt deps
dbt run
dbt test
dbt snapshot
dbt docs generate
```

%md
ğŸ§ª Data Quality Tests

Implemented through dbtâ€™s testing framework:

âœ… unique (IDs)

âœ… not_null

âœ… relationships (FK constraints)

âœ… Source freshness checks

âœ… Snapshot date validity

ğŸ“Š Data Model Summary
Dimensions

DimCustomers

DimDrivers

DimVehicles

DimLocations

DimPayments

DimTrips

Facts

FactTrips (from gold layer)

ğŸ“˜ Documentation (dbt Docs)

To generate:

dbt docs generate
dbt docs serve


Outputs:

Model lineage graph

Column-level documentation

Source metadata

Snapshot history

Test results

ğŸ› ï¸ Tools & Technologies
Layer	Technology
Ingestion	Databricks Notebooks, Spark
Processing	PySpark, Delta Lake
Modeling	dbt Core
Storage	Delta tables (Bronzeâ†’Silverâ†’Gold)
Documentation	dbt docs
Orchestration (optional)	GitHub Actions / Databricks Jobs
ğŸ“ What This Project Demonstrates (for Recruiters)

This repository directly reflects skills needed for:

ğŸ¯ Data Engineer Roles

Medallion architecture

Distributed processing on Spark

Delta Lake ACID operations

Incremental ETL pipelines

SCD Type-2 modeling

ğŸ¯ Analytics Engineer Roles

dbt modeling

Tests, documentation, macros

Fact/dimension design

Data contracts

Reusable transformations

ğŸ¯ Data Analyst Roles

Clean, analytics-ready datasets

Exploratory SQL on Trips / Revenue / Driver metrics

Dashboard screenshots included

ğŸš€ Future Enhancements

 Add Airflow / Prefect orchestration

 Add streaming ingestion (Structured Streaming)

 Add a real-time dashboard using Power BI / Tableau

 Add DuckDB + MotherDuck support

 Add CI/CD for dbt + docs deployment

ğŸ’¬ Contact

If you like this project or want to collaborate, feel free to reach out!


---

%
